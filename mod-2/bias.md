---
title: Humans and machines
layout: default
parent: Module 1
nav_order: 5
---

Isn't it nice that computers are pure blank slates unbound by the politics and biases that plague us humans?

That's actually not the case at all.

## The trouble with algorithms

Computers are programmed by humans who make decisions about what is "normal" when developing software and applications. That means that the biases and misconceptions held by those programmers are likely to have an influence on a system's design. We've seen this happen over and over again, and, as you've read in the "Module 1 overview," there can be really harmful consequences. 

The film *Coded Bias* (2020), directed by Shalini Kantayya, speaks volumes to this problem. *Coded Bias* follows MIT Media Lab researcher Joy Buolamwini's journey uncovering the racial bias coded into facial recognition algorithms and programs. While this summary is a gross oversimplification, essentially, the film explains that facial recognition programs, like any other program, will only recognize input that corresponds to the input it was trained with. So, if an algorithm is trained to recognize faces but is exposed only or primarily to white male faces, it will have trouble processing diverse input. Ultimately, the technology is doing what it's expected to do, it's the people who are teaching it that are presenting the problem.

You can find *Coded Bias* on Netflix, and we highly recommend that you watch it! It's not only informative for our purposes, but it's also a very well done documentary. For a synopsis and in-depth discussion of this film and related issues *with the director*, check out this recording of Geneseo's Black History Month event hosted by Dr. Schacht where he has the opportunity to sit down via Zoom with Kantayya herself!

[Try watching this video on www.youtube.com](https://www.youtube.com/watch?v=S0bq-7k-uqU), or enable JavaScript if it is disabled in your browser.

## Language matters

> One could say that programming became programming and software became software when commands shifted from commanding a "girl" to commanding a machine.... Software languages are based on a series of imperatives that stem from World War II command and control structure (Chun, p. 33).

Remember in the last chapter when we talked about how the first computers were people? It's time now to get more specific. As discussed in detail by Wendy Hui Kyong Chun in ["On Software, or the Persistence of Visual Knowledge"](https://direct.mit.edu/grey/article/doi/10.1162/1526381043320741/10837/On-Software-or-the-Persistence-of-Visual-Knowledge){.inline_disabled target="_blank" rel="noopener"} (2005) almost all computers (as in human computers) in the US during World War II were young women. Human computers received commands from analysts---predominantly men with the military---that they then had to interpret and act upon the machine. Therefore, you'll notice that "the command line", which we'll discuss in more detail later, is laden with masculine and military metaphors, reflecting the history of computing and programming.

> Software, through programming languages that stem from a gendered system of command and control, disciplines its programmers and users, creating an invisible system of visibility (Chun, p. 27).

 This militaristic, masculine, ableist style of interacting with computers has been overlooked in mainstream channels for a very long time, and has even made its way into the general American lexicon. Terms like "blacklist" versus "whitelist", "master" versus "slave." A bit troubling when you look at them like this, yes? What are some alternatives we could use? 

---

"Language matters... especially in tech" is adapted from [Digital Research Institute (DRI) Curriculum](http://purl.org/dc/terms/) by [Graduate Center Digital Initiatives](https://gcdi.commons.gc.cuny.edu/) is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/). 
